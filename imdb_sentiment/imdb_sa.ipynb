{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis in Keras\n",
    "In this notebook we want to train a neural network in Keras in order to predict the sentiment of a movie review, i.e. whether it is positive or negative. For the purpose, the imdb movie review dataset available in Keras is used. This problem has been widely addressed and is one of the example use cases of Keras, useful to experiment and learn.\n",
    "- We rely on the examples provided by the Coursera course https://www.coursera.org/learn/ai/lecture/hQYsN/recurrent-neural-networks, https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/ and http://www.samyzaf.com/ML/imdb/imdb.html\n",
    "- Embeddings are used to represent words and provide them as input to the neural network: https://en.wikipedia.org/wiki/Embedding, https://en.wikipedia.org/wiki/Word_embedding, https://keras.io/layers/embeddings/\n",
    "- Convolutional neural nets were shown leading good results in spite of small network structure: http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/, https://github.com/alexander-rakhlin/CNN-for-Sentence-Classification-in-Keras/blob/master/docs/1408.5882v2.pdf, http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "- A more complete overview at http://cs224d.stanford.edu/syllabus.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "# select max_features most common items from vocabulary\n",
    "max_features = 20000\n",
    "top_words = max_features\n",
    "max_review_length = 500\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=top_words,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=max_review_length,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "(20947,)\n",
      "(20947,)\n"
     ]
    }
   ],
   "source": [
    "print x_train.shape\n",
    "print y_train.shape\n",
    "print x_test.shape\n",
    "print y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the reviews might have different lenght and we have a fixed input for our network to train, we use padding to create sub-sequences of fixed length, i.e. if the raw sequence is longer than the set size it will truncate it, while if shorter than the set size the padding will add zeros to fill the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500)\n",
      "(20947, 500)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "print x_train.shape\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
    "print x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model we use:\n",
    "1. An input Embedding layer using 128-length vectors to represent each word. \n",
    "2. An LSTM layer with 128 memory units\n",
    "3. A Dense output layer with a single neuron, whose sigmoid activation will define the belonging class for the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         2560000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,691,713\n",
      "Trainable params: 2,691,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "#from keras.layers.embeddings import Embedding\n",
    "\n",
    "embedding_vector_length = 128\n",
    "\n",
    "model = Sequential()\n",
    "# input layer\n",
    "model.add(Embedding(max_features,128\n",
    "                    #input_dim=top_words,\n",
    "                    #output_dim=embedding_vector_length,\n",
    "                    #input_length=max_review_length\n",
    "                   ))\n",
    "# hidden layer\n",
    "model.add(LSTM(units=128\n",
    "               #units=128\n",
    "               #dropout=0.2,\n",
    "               #recurrent_dropout=0.2\n",
    "              ))\n",
    "\n",
    "# output layer\n",
    "model.add(Dense(1,\n",
    "                activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              #optimizer='adam',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the seed and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 20947 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 438s 18ms/step - loss: 0.6928 - acc: 0.5107 - val_loss: 0.6928 - val_acc: 0.5336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11c94d5d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "import numpy\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# define the batch size, ie. the size of the set (or number of observations) used between weight updates\n",
    "# this is a tradeoff as bigger batch sizes allow for learning bigger dependencies across data samples\n",
    "# although brings in greater computational complexity\n",
    "# viceversa smaller batch sizer are lighter to handle and might lead to better weight updates for non-sequential data\n",
    "batch_size = 32\n",
    "\n",
    "model.fit(x_train,\n",
    "          y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=1,\n",
    "          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_train, y_train, batch_size=batch_size)\n",
    "print(\"Training: accuracy = %f  ;  loss = %f\" % (accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20947/20947 [==============================] - 67s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6927631945605949, 0.5335847615438776]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(\"Testing: accuracy = %f  ;  loss = %f\" % (accuracy, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imdb.h5       imdb_sa.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "model.save(\"imdb.h5\")\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which can be reloaded as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"imdb.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
